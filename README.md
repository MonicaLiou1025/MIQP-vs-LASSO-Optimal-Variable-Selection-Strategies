# MIQP-vs-LASSO-Optimal-Variable-Selection-Strategies

## Problem Overview
One of the most common problems in predictive analytics is variable selection for regression. Direct variable selection using optimization has long been dismissed by the statistics/analytics community because of computational difficulties. This computational issue was part of the motivation for the development of LASSO and ridge regression. However, in the recent past there have been tremendous advancements in optimization software, specifically the ability to solve mixed integer quadratic programs (MIQP). This project will pose the variable selection problem for regression as an MIQP which you will solve using gurobi. We will compare the results you find to LASSO to see if the additional â€˜shrinkageâ€™ component of LASSO really is more beneficial than finding the â€˜bestâ€™ set of variables to include in your regression.

## Direct Variable Selection â€“ MIQP Problem
Given a dataset of m independent variables, X, and a dependent variable, y, the standard ordinary least squares problem is formulated as minğ›½Î£(ğ›½0+ğ›½1ğ‘¥ğ‘–1+â‹¯+ğ›½ğ‘šğ‘¥ğ‘–ğ‘šâˆ’ğ‘¦ğ‘–)2ğ‘›ğ‘–=1.
In order to incorporate variable selection into this problem we can include some binary variables, zj, that force the corresponding values of ğ›½ğ‘— to be zero if zj is zero, using the big-M method that we discussed in class, and used in the previous project. If we only want to include at most k variables from X, then we can pose this as minğ›½,ğ‘§Î£(ğ›½0+ğ›½1ğ‘¥ğ‘–1+â‹¯+ğ›½ğ‘šğ‘¥ğ‘–ğ‘šâˆ’ğ‘¦ğ‘–)2ğ‘›ğ‘–=1 ğ‘ .ğ‘¡.âˆ’ğ‘€ğ‘§ğ‘—â‰¤ğ›½ğ‘—â‰¤ğ‘€ğ‘§ğ‘— ğ‘“ğ‘œğ‘Ÿ ğ‘—=1,2,3,â€¦,ğ‘š Î£ğ‘§ğ‘—ğ‘šğ‘—=1â‰¤ğ‘˜ ğ‘§ğ‘— ğ‘ğ‘Ÿğ‘’ ğ‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦.
Note that we donâ€™t ever forbid the model from having an intercept term, ğ›½0, and that m and M are different things here. Here, k can be viewed as a hyperparameter to be chosen using cross validation.

## Indirect Variable Selection â€“ LASSO
The LASSO version of regression is posed as minğ›½Î£(ğ›½0+ğ›½1ğ‘¥ğ‘–1+â‹¯+ğ›½ğ‘šğ‘¥ğ‘–ğ‘šâˆ’ğ‘¦ğ‘–)2ğ‘›ğ‘–=1+ğœ†Î£|ğ›½ğ‘—|ğ‘šğ‘—=1,
where ğœ† is a hyperparameter to be chosen using cross-validation. It turns out that if ğœ† is large enough, several values of ğ›½ will be forced to be equal to zero. This model also has the benefit of â€˜shrinkingâ€™ the ğ›½s closer to zero, which achieves variance reduction (prevents overfitting). Note again that ğ›½0 is not included in the ğœ† sum. You should never penalize a model for having an intercept term. The standard package in Python to solve the LASSO problem is scikit learn. In this project you will need to use scikit learn to solve the LASSO problem.
